\documentclass{article}

\usepackage{url}
\usepackage{natbib}

\begin{document}

\paragraph{Manning:} NSF award IIS-1514268 \$1,100,000.00. June 1, 2015--May 31, 2018; no cost extension to May 31, 2019. RI: Medium: Deep Understanding: Integrating Neural and Symbolic Models of Meaning. (Dan Jurafsky, PI; Christopher Manning; Percy Liang)

\subparagraph{Intellectual Merit:} The results of our project have already had a large impact on our ability to use combinations of neural and symbolic approaches to do semantic understanding of human language, including novel models of discourse coherence, models that combine text and networks, novel models of interpretability of embeddings and neural models, new ways of combining adversarial learning with text processing, and novel neural models for conversational dialogue. Particular achievements include the following. We introduced two deep learning algorithms for the task of developing lexicons that predict outcomes like human preferences or actions from text as a way of making these models transparent and interpretable. We investigated the role of linguistic context in an LSTM language model, through ablation studies, analyzing the increase in perplexity when prior context words are shuffled, replaced, or dropped, in order to understand how neural models are able to use much longer context than traditional language models in NLP. By using neural models that combine language and social networks to investigate conflict in communities online, we show that such conflicts tend to be initiated by a handful of communitiesâ€”less than 1\% of communities start 74\% of conflicts. We developed 2 new domain-independent neural models of discourse coherence that are capable of measuring multiple aspects of coherence, a discriminative model that learns to distinguish coherent from incoherent discourse, and generative models that produce coherent text, including a novel neural latent-variable Markovian generative model that captures the latent discourse dependencies between sentences in a text. Our work achieves state-of-the-art performance on multiple coherence evaluations. We addressed the important problem of generating discourse-coherent utterances in dialog by combining reinforcement learning and adversarial training: the system was trained to produce sequences that are indistinguishable from humangenerated dialogue utterances.

\subparagraph{Broader impacts:} 

The project has trained 17 graduate students, postdocs, and undergraduates, including 6 women, who have received weekly mentoring from the PIs in various phases of the project, with training in research methodology, in career development, and in the research content described above. One woman postdoc from this project, Yulia Tsvetkov, has now begun her faculty career at CMU. Three graduate students have graduated: Will Hamilton is starting a faculty job at McGill this year. Raine Hoover received her MS and Jiwei Li received his PhD and both are now at startups. Finally, the undergraduate, Jon Gauthier, graduated and is now a PhD student at MIT.

The algorithms have been described in publications and talks to the community and in talks given by the PIs to computer science departments around the country. Several pieces of code have been incorporated into the Stanford CoreNLP software, which is used by research groups and companies around the country.



\subparagraph{Publications:} Understanding neural models: \cite{li2015tree,khandelwal2018lm}; coreference: \cite{clark2016deep,clark2016improving}; summarization: \cite{see2016compression}; neural language understanding: \cite{bowman2016spinn,hamilton2016cultural,hamilton2016diachronic}; events: \cite{huang2016events}; dialogue: \cite{li2017adversarial,muzny2017twostage,guu2017bridging}; neural sequence translation models: \cite{luong2016acl_hybrid,wuebker2016acl}.


\subparagraph{Research products:}

The algorithms have been described in publications and described in talks to the community and in talks given by the PIs to computer science departments around the country.

 Several systems have been incorporated into the Stanford CoreNLP open source NLP software, which is widely used by research labs and companies around the country, including in particular Kevin Clark's statistical and neural coreference systems: \url{https://stanfordnlp.github.io/coref.html} and Grace/Felix Muzny's quote annotator: \url{https://stanfordnlp.github.io/quote.html}.

Several datasets have been made publicly available, including: The SCONE dataset for learning models of context-dependent executable semantics at: \url{https://nlp.stanford.edu/projects/scone/}; the network model of Reddit interaction at: \url{http://snap.stanford.edu/data/web-RedditNetworks.html}; and quote attribution data: \url{https://stanfordnlp.github.io/quote.html}.


\bibliographystyle{plainnatrev}
\bibliography{manning-previous}

\end{document}
